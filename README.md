# 100layers
## 注意：训练模型前，需要先打开是男人就下一百层（NSSHAFT.exe）这个游戏，然后用Cheat Engine这个工具来查找游戏里一些关键值的内存地址。并修改NSSHAFT第35-39行的值。

nsshaft.py这个文件是用来和游戏交互的。

replaybuffer.py是做经验回放的缓冲池。里边分了两部分，一部分来存放Action value为正的step，一部分用来存放Action value为负的step。每次训练正负两部分里各抽一半。
这样的好处是避免训练时大部分都是Action value为正的step。

DQN.py是主程序。

## 几点需要注意
1. 神经网络的输出是5个神经元，除了向左，不动，向右外，额外两个是输出小人的x，y坐标。训练时用来让网络注意小人的位置。可以加快训练速度。
2. 并不是完整一局的回报值来计算。而是以小人的每一跳的回报值来计算。代码加了额外处理，落在可以翻转的板上也算在空中，直到落到其他板子上才算一跳完成。
3. 神经元前三个神经元的输出，是每个state下，向左，不动，向右三个动作下一跳的期望回报。
